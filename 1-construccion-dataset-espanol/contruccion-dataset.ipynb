{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construcción del dataset\n",
    "\n",
    "En la carpeta `assets` se encuentran dos archivos CSV que contienen información de tweets en Inglés con su respectiva clasificación de sentimiento. El objetivo de este notebook es construir un dataset que contenga la información de ambos archivos. Se seleccionaran de cada archivo 2000 tweets positivos, 2000 tweets negativos y 2000 neutral. Se traducirán los tweets al español y se almacenarán en un archivo CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de librerías\n",
    "%pip install -r '../requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura dataset 1\n",
    "header = ['id', 'entity', 'sentiment', 'text']\n",
    "dataset1_no_procesado = pd.read_csv('./assets/dataset1-no-procesado.csv', delimiter=',', encoding='utf-8', names=header)\n",
    "# Descrición dataset\n",
    "print(dataset1_no_procesado.describe())\n",
    "# Visualización primeros registros\n",
    "dataset1_no_procesado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza dataset 1\n",
    "dataset1_procesado = dataset1_no_procesado.dropna()\n",
    "dataset1_procesado = dataset1_procesado.drop_duplicates(subset='text')\n",
    "dataset1_procesado = dataset1_procesado[dataset1_procesado['text'] != '']\n",
    "dataset1_procesado = dataset1_procesado[(dataset1_procesado['sentiment'] == 'Positive') | \n",
    "                                        (dataset1_procesado['sentiment'] == 'Negative') | \n",
    "                                        (dataset1_procesado['sentiment'] == 'Neutral')]\n",
    "# Descrición dataset\n",
    "print(dataset1_procesado.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de columnas\n",
    "dataset1_procesado = dataset1_procesado[['text', 'sentiment']]\n",
    "dataset1_procesado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección aleatoria de 5000 para cada clase\n",
    "dataset1_procesado = dataset1_procesado.groupby('sentiment').apply(lambda x: x.sample(n=5000)).reset_index(drop=True)\n",
    "# Descripción dataset\n",
    "print(dataset1_procesado['sentiment'].value_counts())\n",
    "print(dataset1_procesado['text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura dataset 2\n",
    "dataset2_no_procesado = pd.read_csv('./assets/dataset2-no-procesado.csv', delimiter=',', encoding='unicode_escape')\n",
    "# Descrición dataset\n",
    "print(dataset2_no_procesado.describe())\n",
    "# Visualización primeros registros\n",
    "dataset2_no_procesado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza dateset 2\n",
    "dataset2_procesado = dataset2_no_procesado.dropna()\n",
    "dataset2_procesado = dataset2_procesado[dataset2_procesado['selected_text'] != '']\n",
    "dataset2_procesado = dataset2_procesado.drop_duplicates(subset='selected_text')\n",
    "dataset2_procesado = dataset2_procesado[(dataset2_procesado['sentiment'] == 'positive') | \n",
    "                                        (dataset2_procesado['sentiment'] == 'negative') | \n",
    "                                        (dataset2_procesado['sentiment'] == 'neutral')]\n",
    "# Descrición dataset\n",
    "print(dataset1_procesado.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de columnas\n",
    "dataset2_procesado = dataset2_procesado[['selected_text', 'sentiment']]\n",
    "# Renombrar columnas\n",
    "dataset2_procesado.columns = ['text', 'sentiment']\n",
    "dataset2_procesado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección aleatoria de 5000 para cada clase\n",
    "dataset2_procesado = dataset2_procesado.groupby('sentiment').apply(lambda x: x.sample(n=5000)).reset_index(drop=True)\n",
    "# Descripción dataset\n",
    "print(dataset2_procesado['sentiment'].value_counts())\n",
    "print(dataset2_procesado['text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Juntar los dos datasets en un solo archivo CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unión de datasets\n",
    "dataset = pd.concat([dataset1_procesado, dataset2_procesado])\n",
    "# Dejar clases en minúsculas\n",
    "dataset['sentiment'] = dataset['sentiment'].str.lower()\n",
    "# Descripción dataset\n",
    "print(dataset['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportación dataset 30000 registros\n",
    "# train  = 80% = 24000 registros\n",
    "# test = 20% = 6000 registros ->\n",
    "dataset_train, dataset_other = train_test_split(dataset, test_size=0.2, random_state=42, stratify=dataset['sentiment'])\n",
    "dataset_test, dataset_validation = train_test_split(dataset_other, test_size=0.5, random_state=42, stratify=dataset_other['sentiment'])\n",
    "dataset_train.to_csv('./dist/dataset-english-tweets-train.csv', index=False, encoding='utf-8')\n",
    "dataset_test.to_csv('./dist/dataset-english-tweets-test.csv', index=False, encoding='utf-8')\n",
    "dataset_validation.to_csv('./dist/dataset-english-tweets-validation.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total registros:', dataset.shape[0])\n",
    "print('Train registros:', dataset_train.shape[0])\n",
    "print('Test registros:', dataset_test.shape[0])\n",
    "print('Validation registros:', dataset_validation.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-talentotech-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
