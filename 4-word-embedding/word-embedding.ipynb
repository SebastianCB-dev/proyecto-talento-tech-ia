{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "\n",
    "En este notebook se creará un modelo de Word Embedding utilizando la librería Gensim. Se utilizará el dataset de entrenamiento de la competencia para entrenar el modelo. Luego, se utilizará el modelo para obtener los embeddings de las palabras de los textos de los tweets y se guardarán en un archivo para utilizarlos en los modelos de clasificación.\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:678/1*5F4TXdFYwqi-BWTToQPIfg.jpeg\" width=\"500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de librerías\n",
    "%pip install -r '../requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías\n",
    "import pandas as pd\n",
    "import json\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos de entrenamiento\n",
    "# Se utilizará el dataset de entrenamiento para entrenar el modelo\n",
    "dataset_entrenamiento = pd.read_csv('../3-NLP/dist/dataset-spanish-tweets-train-preprocessed.csv', sep=',', encoding='utf-8')\n",
    "dataset_entrenamiento['text'] = dataset_entrenamiento['text'].apply(lambda x: json.loads(x))\n",
    "dataset_entrenamiento.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar arreglos con longitud 0\n",
    "dataset_entrenamiento = dataset_entrenamiento[dataset_entrenamiento['text'].apply(lambda x: len(x) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_entrenamiento.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición del Word Embedding\n",
    "\n",
    "El Word Embedding es una técnica de procesamiento de lenguaje natural que consiste en asignar una representación vectorial a cada palabra de un vocabulario. Esta representación vectorial se obtiene a partir de un modelo de lenguaje entrenado con un corpus de texto. Los vectores obtenidos tienen la propiedad de que palabras similares en el contexto de los textos tienen vectores similares.\n",
    "\n",
    "La idea es vectorizar cada una de las palabras en un vector de 150 dimensiones, luego hacer un promedio de los vectores de todas las palabras de un tweet para obtener un vector de 150 dimensiones que represente al tweet.\n",
    "\n",
    "Cada dimensión del vector representa una característica de la palabra, por lo que palabras similares tendrán vectores similares.\n",
    "\n",
    "Nuestro conjunto de datos al final será una matriz de 150 columnas, donde cada fila representa un tweet. Y por supuesto otra columna con la etiqueta del tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparámetros\n",
    "\n",
    "1. sentences: lista de listas de palabras. Cada lista de palabras representa un tweet.\n",
    "2. vector_size: tamaño del vector de embeddings. Aquí se define el número de dimensiones del vector de embeddings. Cuanto mayor sea el número, más información se podrá capturar, pero también será más costoso computacionalmente.\n",
    "3. window: tamaño de la ventana de contexto. Es el número de palabras que se consideran a la izquierda y a la derecha de la palabra objetivo.\n",
    "4. min_count: número mínimo de veces que una palabra debe aparecer en el corpus para ser considerada en el modelo.\n",
    "5. workers: número de núcleos de la CPU a utilizar para entrenar el modelo.\n",
    "6. sg: algoritmo de entrenamiento. 1 para Skip-gram, 0 para CBOW.\n",
    "7. epochs: número de iteraciones sobre el corpus.\n",
    "8. seed: semilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = dataset_entrenamiento['text'].tolist()\n",
    "vector_size = 150\n",
    "window = 10\n",
    "min_count = 1\n",
    "workers = 4\n",
    "sg = 1 # Skip Gram tiene mejor rendimiento frente a CBOW\n",
    "epochs = 60\n",
    "seed = 42 # Para reproducibilidad\n",
    "model = Word2Vec(sentences=sentences,\n",
    "                 vector_size=vector_size,\n",
    "                 window=window,\n",
    "                 min_count=min_count,\n",
    "                 workers=workers,\n",
    "                 sg=sg,\n",
    "                 epochs=epochs,\n",
    "                 seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una vez entrenado el modelo, se guarda en disco\n",
    "model.save('./dist/word2vec.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-talentotech-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
