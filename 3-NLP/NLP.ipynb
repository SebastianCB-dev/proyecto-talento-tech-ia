{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Natural Language Processing (Procesamiento de Lenguaje Natural)\n",
    "\n",
    "En este notebook se hará el procesamiento de lenguaje natural (NLP) en los comentarios de los datasets de entrenamiento y prueba. Se utilizará la librería `nltk` para realizar el procesamiento de los comentarios.\n",
    "\n",
    "Lo que se hará es:\n",
    "- Convertir los comentarios a minúsculas\n",
    "- Tokenizar los comentarios\n",
    "- Eliminar contenido que hace ruido -> URLs, menciones, hashtags, etc.\n",
    "- Eliminar las palabras vacías (stopwords)\n",
    "- Lematizar las palabras\n",
    "- Eliminar los signos de puntuación\n",
    "- Eliminar los números\n",
    "- Eliminar los emojis\n",
    "- Eliminar los espacios en blanco\n",
    "\n",
    "Luego una vez ya queden los tokens limpios, se entrenará un Word2Vec para obtener los vectores de palabras de los comentarios. (Promedio de los vectores de palabras de cada comentario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==2.2.2 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from -r ../requirements.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn==1.5.1 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from -r ../requirements.txt (line 2)) (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from pandas==2.2.2->-r ../requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from pandas==2.2.2->-r ../requirements.txt (line 1)) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from pandas==2.2.2->-r ../requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from pandas==2.2.2->-r ../requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from scikit-learn==1.5.1->-r ../requirements.txt (line 2)) (1.14.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from scikit-learn==1.5.1->-r ../requirements.txt (line 2)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from scikit-learn==1.5.1->-r ../requirements.txt (line 2)) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2->-r ../requirements.txt (line 1)) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Instalación de librerías\n",
    "%pip install -r '../requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de clase para el preprocesamiento de datos\n",
    "\n",
    "class Preprocesamiento:\n",
    "  stopwords = []\n",
    "\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def preprocesar_texto(self, texto: str) -> list[str]:\n",
    "    nuevo_texto = self.pasar_a_minusculas(texto)\n",
    "    tokens = self.tokenizar_texto(nuevo_texto)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "  # Paso 1: Convertir el texto a minúsculas\n",
    "  def pasar_a_minusculas(self, texto: str) -> str:\n",
    "    return texto.lower()\n",
    "\n",
    "  # Paso 2: Tokenizar el texto\n",
    "  # Ejemplo: \"Hola, ¿cómo estás?\" -> [\"Hola,\", \"¿cómo\", \"estás?\"]\n",
    "  def tokenizar_texto(self, texto: str) -> list[str]:\n",
    "    texto_tokenizado = texto.split(' ')\n",
    "    texto_tokenizado = self.limpiar_tokens(texto_tokenizado)\n",
    "    texto_tokenizado_limpio = self.eliminar_ruido(texto_tokenizado)\n",
    "    return texto_tokenizado_limpio\n",
    "\n",
    "  # Paso 3: Eliminar ruido como URLs, menciones y hashtags\n",
    "  # Ejemplo: [\"Hola,\"@sebastian, \"http://www.google.com\", \"#InteligenciaArtificial\", \"¿cómo\", \"estás?\"] -> [\"Hola,\", \"¿cómo\", \"estás?\"]\n",
    "  def eliminar_ruido(self, tokens: list[str]) -> list[str]:\n",
    "    # Eliminar URLs\n",
    "    lista_limpia = []\n",
    "    for token in tokens:\n",
    "      if not token.startswith('http'):\n",
    "        lista_limpia.append(token)\n",
    "    # Eliminar Hashtags\n",
    "    lista_limpia = [token for token in lista_limpia if not token.startswith('#')]\n",
    "    # Eliminar menciones\n",
    "    lista_limpia = [token for token in lista_limpia if not token.startswith('@')]\n",
    "    return lista_limpia\n",
    "\n",
    "  # Paso 4: Eliminar stopwords\n",
    "  def eliminar_stopwords(self, tokens: list[str]) -> list[str]:\n",
    "    tokens_limpio = [token for token in tokens if token not in self.stopwords]\n",
    "    return\n",
    "\n",
    "  # Helper 1: Limpiar tokens\n",
    "  # Ejemplo: [\"Hola,\", \"\", \"¿cómo\", \"estás?\"] -> [\"Hola,\", \"¿cómo\", \"estás?\"]\n",
    "  def limpiar_tokens(self, tokens: list[str]) -> list[str]:\n",
    "    tokens_limpio = [token for token in tokens if token != '']\n",
    "    return tokens_limpio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comentario original: ¡Odioooooooooooooo el League of Legends porque  es  un  juego muy tóxico y la comunidad es muy mala! 😝 el link para descargarlo es https://www.leagueoflegends.com\n",
      "Comentario preprocesado: ['¡odioooooooooooooo', 'el', 'league', 'of', 'legends', 'porque', 'es', 'un', 'juego', 'muy', 'tóxico', 'y', 'la', 'comunidad', 'es', 'muy', 'mala!', '😝', 'el', 'link', 'para', 'descargarlo', 'es']\n"
     ]
    }
   ],
   "source": [
    "# Preprocesamiento de datos\n",
    "preprocesamiento = Preprocesamiento()\n",
    "comment = '¡Odioooooooooooooo el League of Legends porque  es  un  juego muy tóxico y la comunidad es muy mala! 😝 el link para descargarlo es https://www.leagueoflegends.com'\n",
    "comment_preprocesado = preprocesamiento.preprocesar_texto(comment)\n",
    "print(f'Comentario original: {comment}')\n",
    "print(f'Comentario preprocesado: {comment_preprocesado}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-talentotech-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
