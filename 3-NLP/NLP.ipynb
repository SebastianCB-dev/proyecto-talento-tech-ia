{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP - Natural Language Processing (Procesamiento de Lenguaje Natural)\n",
    "\n",
    "En este notebook se hará el procesamiento de lenguaje natural (NLP) en los comentarios de los datasets de entrenamiento y prueba. Se utilizará la librería `nltk` para realizar el procesamiento de los comentarios.\n",
    "\n",
    "Lo que se hará es:\n",
    "1. Convertir los comentarios a minúsculas\n",
    "2. Tokenizar los comentarios\n",
    "3. Eliminar contenido que hace ruido -> URLs, menciones, hashtags, etc.\n",
    "4. Reducir palabras extendidas (como \"Holaaaaaaa\" a \"Hola\")\n",
    "5. Eliminar las palabras vacías (stopwords)\n",
    "6. Lematizar las palabras\n",
    "7. Eliminar los signos de puntuación\n",
    "8. Eliminar los números\n",
    "9. Eliminar los emojis\n",
    "10. Eliminar los espacios en blanco\n",
    "\n",
    "Luego una vez ya queden los tokens limpios, se entrenará un Word2Vec para obtener los vectores de palabras de los comentarios. (Promedio de los vectores de palabras de cada comentario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==2.2.2 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from -r ../requirements.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn==1.5.1 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from -r ../requirements.txt (line 2)) (1.5.1)\n",
      "Requirement already satisfied: nltk==3.8.2 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from -r ../requirements.txt (line 3)) (3.8.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from pandas==2.2.2->-r ../requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from pandas==2.2.2->-r ../requirements.txt (line 1)) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from pandas==2.2.2->-r ../requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from pandas==2.2.2->-r ../requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from scikit-learn==1.5.1->-r ../requirements.txt (line 2)) (1.14.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from scikit-learn==1.5.1->-r ../requirements.txt (line 2)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from scikit-learn==1.5.1->-r ../requirements.txt (line 2)) (3.5.0)\n",
      "Requirement already satisfied: click in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from nltk==3.8.2->-r ../requirements.txt (line 3)) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from nltk==3.8.2->-r ../requirements.txt (line 3)) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from nltk==3.8.2->-r ../requirements.txt (line 3)) (4.66.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2->-r ../requirements.txt (line 1)) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Instalación de librerías\n",
    "%pip install -r '../requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías\n",
    "# Expresiones regulares\n",
    "import re\n",
    "# Manejo de datos\n",
    "import pandas as pd\n",
    "# Procesamiento de texto\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos\n",
    "# datos_entrenamiento = pd.read_csv('../2-traduccion-dataset/result/dataset-spanish-tweets-train.csv', delimiter=',', encoding='utf-8')\n",
    "# datos_prueba = pd.read_csv('../2-traduccion-dataset/result/dataset-spanish-tweets-test.csv', delimiter=',', encoding='utf-8')\n",
    "# datos_validacion = pd.read_csv('../2-traduccion-dataset/result/dataset-spanish-tweets-validation.csv', delimiter=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de clase para el preprocesamiento de datos\n",
    "\n",
    "class Preprocesamiento:\n",
    "  stopwords_es = []\n",
    "  stopwords_en = []\n",
    "\n",
    "  def __init__(self):\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt_tab')\n",
    "    self.stopwords_es = stopwords.words('spanish')\n",
    "    self.stopwords_en = stopwords.words('english')\n",
    "    pass\n",
    "\n",
    "  def preprocesar_texto(self, texto: str) -> list[str]:\n",
    "    nuevo_texto = self.pasar_a_minusculas(texto)\n",
    "    tokens = self.tokenizar_texto(nuevo_texto)\n",
    "    tokens = self.eliminar_ruido(tokens)\n",
    "    tokens = self.reducir_palabras_extendidas(tokens)\n",
    "    tokens = self.eliminar_stopwords(tokens)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "  # Paso 1: Convertir el texto a minúsculas\n",
    "  def pasar_a_minusculas(self, texto: str) -> str:\n",
    "    return texto.lower()\n",
    "\n",
    "  # Paso 2: Tokenizar el texto\n",
    "  # Ejemplo: \"Hola, ¿cómo estás?\" -> [\"Hola,\", \"¿cómo\", \"estás?\"]\n",
    "  def tokenizar_texto(self, texto: str) -> list[str]:\n",
    "    texto_tokenizado = texto.split(' ')\n",
    "    texto_tokenizado = self.limpiar_tokens(texto_tokenizado)\n",
    "    texto_tokenizado_limpio = self.eliminar_ruido(texto_tokenizado)\n",
    "    texto_tokenizado_limpio = self.reducir_palabras_extendidas(texto_tokenizado_limpio)\n",
    "    return texto_tokenizado_limpio\n",
    "\n",
    "  # Paso 3: Eliminar ruido como URLs, menciones y hashtags\n",
    "  # Ejemplo: [\"Hola,\"@sebastian, \"http://www.google.com\", \"#InteligenciaArtificial\", \"¿cómo\", \"estás?\"] -> [\"Hola,\", \"¿cómo\", \"estás?\"]\n",
    "  def eliminar_ruido(self, tokens: list[str]) -> list[str]:\n",
    "    # Eliminar URLs\n",
    "    lista_limpia = []\n",
    "    for token in tokens:\n",
    "      if not token.startswith('http'):\n",
    "        lista_limpia.append(token)\n",
    "    # Eliminar Hashtags\n",
    "    lista_limpia = [token for token in lista_limpia if not token.startswith('#')]\n",
    "    # Eliminar menciones\n",
    "    lista_limpia = [token for token in lista_limpia if not token.startswith('@')]\n",
    "    return lista_limpia\n",
    "\n",
    "  # Paso 4: Reducir palabras extendidas\n",
    "  # Ejemplo: [\"Holaaaa\"] -> [\"Hola\"]\n",
    "  def reducir_palabras_extendidas(self, tokens: list[str]) -> list[str]:\n",
    "    for i in range(len(tokens)):\n",
    "      tokens[i] = re.sub(r'(.)\\1{2,}', r'\\1\\1', tokens[i])\n",
    "    return tokens\n",
    "\n",
    "  # Paso 5: Eliminar stopwords\n",
    "  def eliminar_stopwords(self, tokens: list[str]) -> list[str]:\n",
    "    tokens_limpio = [token for token in tokens if token not in self.stopwords_es]\n",
    "    tokens_limpio = [token for token in tokens if token not in self.stopwords_en]\n",
    "    return tokens_limpio\n",
    "\n",
    "  # Helper 1: Limpiar tokens\n",
    "  # Ejemplo: [\"Hola,\", \"\", \"¿cómo\", \"estás?\"] -> [\"Hola,\", \"¿cómo\", \"estás?\"]\n",
    "  def limpiar_tokens(self, tokens: list[str]) -> list[str]:\n",
    "    tokens_limpio = [token for token in tokens if token != '']\n",
    "    return tokens_limpio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: HTTP Error 500: Internal Server\n",
      "[nltk_data]     Error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comentario original: ¡Odioooooooooooooo el League of Legends porque  es  un  juego muy tóxico y la comunidad es muy mala! 😝 el link para descargarlo es https://www.leagueoflegends.com\n",
      "Comentario preprocesado: ['¡odioo', 'el', 'league', 'legends', 'porque', 'es', 'un', 'juego', 'muy', 'tóxico', 'la', 'comunidad', 'es', 'muy', 'mala!', '😝', 'el', 'link', 'para', 'descargarlo', 'es']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt_tab: HTTP Error 500: Internal Server\n",
      "[nltk_data]     Error\n"
     ]
    }
   ],
   "source": [
    "# Preprocesamiento de datos\n",
    "preprocesamiento = Preprocesamiento()\n",
    "comment = '¡Odioooooooooooooo el League of Legends porque  es  un  juego muy tóxico y la comunidad es muy mala! 😝 el link para descargarlo es https://www.leagueoflegends.com'\n",
    "comment_preprocesado = preprocesamiento.preprocesar_texto(comment)\n",
    "print(f'Comentario original: {comment}')\n",
    "print(f'Comentario preprocesado: {comment_preprocesado}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-talentotech-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
