{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Natural Language Processing (Procesamiento de Lenguaje Natural)\n",
    "\n",
    "En este notebook se har치 el procesamiento de lenguaje natural (NLP) en los comentarios de los datasets de entrenamiento y prueba. Se utilizar치 la librer칤a `nltk` para realizar el procesamiento de los comentarios.\n",
    "\n",
    "Lo que se har치 es:\n",
    "- Convertir los comentarios a min칰sculas\n",
    "- Tokenizar los comentarios\n",
    "- Eliminar contenido que hace ruido -> URLs, menciones, hashtags, etc.\n",
    "- Eliminar las palabras vac칤as (stopwords)\n",
    "- Lematizar las palabras\n",
    "- Eliminar los signos de puntuaci칩n\n",
    "- Eliminar los n칰meros\n",
    "- Eliminar los emojis\n",
    "- Eliminar los espacios en blanco\n",
    "\n",
    "Luego una vez ya queden los tokens limpios, se entrenar치 un Word2Vec para obtener los vectores de palabras de los comentarios. (Promedio de los vectores de palabras de cada comentario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==2.2.2 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from -r ../requirements.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn==1.5.1 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from -r ../requirements.txt (line 2)) (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from pandas==2.2.2->-r ../requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from pandas==2.2.2->-r ../requirements.txt (line 1)) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from pandas==2.2.2->-r ../requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from pandas==2.2.2->-r ../requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from scikit-learn==1.5.1->-r ../requirements.txt (line 2)) (1.14.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from scikit-learn==1.5.1->-r ../requirements.txt (line 2)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from scikit-learn==1.5.1->-r ../requirements.txt (line 2)) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/sebastiancb/anaconda3/envs/env-talentotech-project/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2->-r ../requirements.txt (line 1)) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Instalaci칩n de librer칤as\n",
    "%pip install -r '../requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaci칩n de librer칤as\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definici칩n de clase para el preprocesamiento de datos\n",
    "\n",
    "class Preprocesamiento:\n",
    "  stopwords = []\n",
    "\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def preprocesar_texto(self, texto: str) -> list[str]:\n",
    "    nuevo_texto = self.pasar_a_minusculas(texto)\n",
    "    tokens = self.tokenizar_texto(nuevo_texto)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "  # Paso 1: Convertir el texto a min칰sculas\n",
    "  def pasar_a_minusculas(self, texto: str) -> str:\n",
    "    return texto.lower()\n",
    "\n",
    "  # Paso 2: Tokenizar el texto\n",
    "  # Ejemplo: \"Hola, 쯖칩mo est치s?\" -> [\"Hola,\", \"쯖칩mo\", \"est치s?\"]\n",
    "  def tokenizar_texto(self, texto: str) -> list[str]:\n",
    "    texto_tokenizado = texto.split(' ')\n",
    "    texto_tokenizado = self.limpiar_tokens(texto_tokenizado)\n",
    "    texto_tokenizado_limpio = self.eliminar_ruido(texto_tokenizado)\n",
    "    return texto_tokenizado_limpio\n",
    "\n",
    "  # Paso 3: Eliminar ruido como URLs, menciones y hashtags\n",
    "  # Ejemplo: [\"Hola,\"@sebastian, \"http://www.google.com\", \"#InteligenciaArtificial\", \"쯖칩mo\", \"est치s?\"] -> [\"Hola,\", \"쯖칩mo\", \"est치s?\"]\n",
    "  def eliminar_ruido(self, tokens: list[str]) -> list[str]:\n",
    "    # Eliminar URLs\n",
    "    lista_limpia = []\n",
    "    for token in tokens:\n",
    "      if not token.startswith('http'):\n",
    "        lista_limpia.append(token)\n",
    "    # Eliminar Hashtags\n",
    "    lista_limpia = [token for token in lista_limpia if not token.startswith('#')]\n",
    "    # Eliminar menciones\n",
    "    lista_limpia = [token for token in lista_limpia if not token.startswith('@')]\n",
    "    return lista_limpia\n",
    "\n",
    "  # Paso 4: Eliminar stopwords\n",
    "  def eliminar_stopwords(self, tokens: list[str]) -> list[str]:\n",
    "    tokens_limpio = [token for token in tokens if token not in self.stopwords]\n",
    "    return\n",
    "\n",
    "  # Helper 1: Limpiar tokens\n",
    "  # Ejemplo: [\"Hola,\", \"\", \"쯖칩mo\", \"est치s?\"] -> [\"Hola,\", \"쯖칩mo\", \"est치s?\"]\n",
    "  def limpiar_tokens(self, tokens: list[str]) -> list[str]:\n",
    "    tokens_limpio = [token for token in tokens if token != '']\n",
    "    return tokens_limpio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comentario original: 춰Odioooooooooooooo el League of Legends porque  es  un  juego muy t칩xico y la comunidad es muy mala! 游땧 el link para descargarlo es https://www.leagueoflegends.com\n",
      "Comentario preprocesado: ['춰odioooooooooooooo', 'el', 'league', 'of', 'legends', 'porque', 'es', 'un', 'juego', 'muy', 't칩xico', 'y', 'la', 'comunidad', 'es', 'muy', 'mala!', '游땧', 'el', 'link', 'para', 'descargarlo', 'es']\n"
     ]
    }
   ],
   "source": [
    "# Preprocesamiento de datos\n",
    "preprocesamiento = Preprocesamiento()\n",
    "comment = '춰Odioooooooooooooo el League of Legends porque  es  un  juego muy t칩xico y la comunidad es muy mala! 游땧 el link para descargarlo es https://www.leagueoflegends.com'\n",
    "comment_preprocesado = preprocesamiento.preprocesar_texto(comment)\n",
    "print(f'Comentario original: {comment}')\n",
    "print(f'Comentario preprocesado: {comment_preprocesado}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-talentotech-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
