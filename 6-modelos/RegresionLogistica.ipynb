{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos\n",
    "\n",
    "En este notebook se evaluarán los siguientes modelos:\n",
    "1. Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de librerías\n",
    "%pip install -r '../requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos\n",
    "dataset_entrenamiento = pd.read_csv('../5-dataset-final/dist/dataset-train.csv')\n",
    "dataset_pruebas = pd.read_csv('../5-dataset-final/dist/dataset-test.csv')\n",
    "dataset_validacion = pd.read_csv('../5-dataset-final/dist/dataset-validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar valores nulos\n",
    "dataset_entrenamiento = dataset_entrenamiento.dropna()\n",
    "dataset_pruebas = dataset_pruebas.dropna()\n",
    "dataset_validacion = dataset_validacion.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver cantidad de registros\n",
    "print(f'Dataset de entrenamiento: {len(dataset_entrenamiento)} registros')\n",
    "print(f'Dataset de pruebas: {len(dataset_pruebas)} registros')\n",
    "print(f'Dataset de validación: {len(dataset_validacion)} registros')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# División de datos\n",
    "# Entrenamiento\n",
    "X_train = dataset_entrenamiento.drop(columns=['sentiment'])\n",
    "y_train = dataset_entrenamiento['sentiment']\n",
    "# Pruebas\n",
    "X_test = dataset_pruebas.drop(columns=['sentiment'])\n",
    "y_test = dataset_pruebas['sentiment']\n",
    "# Validación\n",
    "X_val = dataset_validacion.drop(columns=['sentiment'])\n",
    "y_val = dataset_validacion['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos de machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid para encontrar el mejor max_iter\n",
    "modelo = LogisticRegression()\n",
    "# Definir los parámetros para la búsqueda\n",
    "param_grid = {'max_iter': [100, 200, 500, 1000, 2000]}\n",
    "\n",
    "# Realizar la búsqueda de cuadrícula con validación cruzada\n",
    "grid_search = GridSearchCV(modelo, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mejor parámetro encontrado\n",
    "best_max_iter = grid_search.best_params_['max_iter']\n",
    "print(f\"El mejor valor de max_iter es: {best_max_iter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resultado\n",
    "\n",
    "Según el GridSearchCV, el mejor valor para el hiperparámetro max_iter es 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia del modelo\n",
    "modelo = LogisticRegression(max_iter=best_max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento\n",
    "modelo.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = modelo.predict(X_train)\n",
    "\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "print(f'Accuracy: {accuracy_train}')\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "print(confusion_matrix(y_train, y_pred_train))\n",
    "print(f'F1 Score: {f1_score(y_train, y_pred_train)}')\n",
    "print(f'Recall: {recall_score(y_train, y_pred_train)}')\n",
    "print(f'Precision: {precision_score(y_train, y_pred_train)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruebas\n",
    "y_pred_test = modelo.predict(X_test)\n",
    "\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "print(confusion_matrix(y_test, y_pred_test))\n",
    "print(f'Accuracy: {accuracy_test}')\n",
    "print(f'F1 Score: {f1_score(y_test, y_pred_test)}')\n",
    "print(f'Recall: {recall_score(y_test, y_pred_test)}')\n",
    "print(f'Precision: {precision_score(y_test, y_pred_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validación\n",
    "y_pred_val = modelo.predict(X_val)\n",
    "\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "print(confusion_matrix(y_val, y_pred_val))\n",
    "print(f'Accuracy: {accuracy_val}')\n",
    "print(f'F1 Score: {f1_score(y_val, y_pred_val)}')\n",
    "print(f'Recall: {recall_score(y_val, y_pred_val)}')\n",
    "print(f'Precision: {precision_score(y_val, y_pred_val)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-talentotech-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
